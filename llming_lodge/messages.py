"""Message type definitions for LLM responses."""
from typing import Optional, List

from pydantic import Field

from .llm_base_models import ChatMessage, Role
from .tools.tool_call import ToolCallInfo, ToolCallStatus


class LlmBaseMessage(ChatMessage):
    """Base class for all LLM message types."""
    response_metadata: Optional[dict] = Field(default_factory=dict, description="Response metadata from the LLM")


class LlmSystemMessage(LlmBaseMessage):
    """System message type."""
    role: Role = Field(Role.SYSTEM, description="System role identifier")


class LlmHumanMessage(LlmBaseMessage):
    """Human message type."""
    role: Role = Field(Role.USER, description="Human role identifier")
    images: Optional[List[str]] = Field(default=None, description="Base64-encoded images to include with message")


class LlmAIMessage(LlmBaseMessage):
    """AI message type."""
    role: Role = Field(Role.ASSISTANT, description="AI assistant role identifier")
    images: Optional[List[str]] = Field(default=None, description="Base64-encoded images generated by the assistant")


class LlmMessageChunk(ChatMessage):
    """Chunk of a message for streaming responses.

    For text content: content contains text, tool_call is None
    For tool events: content may be empty, tool_call contains structured info
    For multimodal responses: images may contain base64 data URIs
    """
    index: int = Field(description="Index of this chunk in the sequence")
    is_final: bool = Field(description="Whether this is the final chunk")
    response_metadata: Optional[dict] = Field(default_factory=dict, description="Response metadata from the LLM")
    tool_call: Optional[ToolCallInfo] = Field(default=None, description="Tool call information if this chunk is a tool event")
    images: Optional[List[str]] = Field(default=None, description="Base64 data URIs for images in multimodal responses")
